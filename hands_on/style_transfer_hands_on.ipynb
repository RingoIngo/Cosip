{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import pdb\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import model \n",
    "    \n",
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"models_path\",'vgg16_weights.npz', \"\")\n",
    "flags.DEFINE_string(\"content_image_path\",'images/tubingen.jpg', \"\")\n",
    "flags.DEFINE_string(\"style_image_path\",'images/starry-night.jpg', \"\")\n",
    "flags.DEFINE_float(\"content_loss_weight\",0.999, \"\")\n",
    "flags.DEFINE_float(\"style_loss_weight\",1e-3, \"\")\n",
    "flags.DEFINE_integer(\"max_steps\",100, \"\")\n",
    "flags.DEFINE_float(\"learning_rate\",0.1, \"\")\n",
    "FLAGS = flags.FLAGS\n",
    "print('Imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "VGG16 is a pretrained model which has been trained on 1.2million images\n",
    "Given below is the architecture of VGG16\n",
    "conv referes to convolutional layers \n",
    "fc refers to fully connected layers\n",
    "'''\n",
    "#################################################################################\n",
    "vgg_layers = ['conv1_1', 'conv1_2', 'pool1', \n",
    "              'conv2_1', 'conv2_2', 'pool2', \n",
    "              'conv3_1', 'conv3_2', 'conv3_3', 'pool3',\n",
    "              'conv4_1', 'conv4_2', 'conv4_3', 'pool4',\n",
    "              'conv5_1', 'conv5_2', 'conv5_3', 'pool5',\n",
    "              'fc6', 'fc7', 'fc8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "Function which computes the gram matrix given a vector tensor \n",
    "'''\n",
    "#################################################################################\n",
    "def gram_matrix(feature_maps):\n",
    "  \"\"\"Computes the Gram matrix for a set of feature maps.\"\"\"\n",
    "  batch_size, height, width, channels = tf.unstack(tf.shape(feature_maps))\n",
    "  denominator = tf.to_float(height * width)\n",
    "  feature_maps = tf.reshape(\n",
    "      feature_maps, tf.stack([batch_size, height * width, channels]))\n",
    "  matrix = tf.matmul(feature_maps, feature_maps, adjoint_a=True)\n",
    "  return matrix / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "'''\n",
    "Read the content and style images \n",
    "The images are of shape: 224,224,3 #RGB channels \n",
    "'''\n",
    "#################################################################################\n",
    "    \n",
    "# Open content image and resize it \n",
    "content_image = imread(FLAGS.content_image_path)[:,:,:3]\n",
    "content_image = np.expand_dims(imresize(content_image, (224, 224)), 0)\n",
    "    \n",
    "# Open style image and resize it \n",
    "style_image = imread(FLAGS.style_image_path)[:,:,:3]\n",
    "style_image = np.expand_dims(imresize(style_image, (224, 224)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "Create Graph first by defining placeholders, ops, loss functions and optimizers\n",
    "\n",
    "'''\n",
    "#################################################################################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "'''Placeholders'''\n",
    "#################################################################################\n",
    "content_tensor = '''Fill your code here''' #shape=[1,224,224,3] \n",
    "style_tensor = '''Fill your code here''' #shape=[1,224,224,3] \n",
    "    \n",
    "                   \n",
    "# Define the output tensor as a Variable - so we can train it\n",
    "# The output tensor should be initialized with the content image\n",
    "#################################################################################\n",
    "'''Output Tensor - initialize with content image - which we will train'''\n",
    "#################################################################################    \n",
    "output_tensor = tf.Variable('''Fill your code here''') #shape=[1,224,224,3] \n",
    "    \n",
    "\n",
    "#################################################################################\n",
    "'''\n",
    "The class object for the VGG16 neural network\n",
    "Propagate the content_tensor, style_tensor and the output tensor through this model\n",
    "'''\n",
    "#################################################################################    \n",
    "vgg = model.vgg16()\n",
    "        \n",
    "# Forward pass the content tensors\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    content_end_points = vgg.forward(content_tensor)\n",
    "    # content_end_points are a dictionary of (key, value) pairs with \n",
    "    # keys=layer_names and values=activationsis a dictionary of \n",
    "        \n",
    "# Forward pass the style tensors - make sure to reuse the variables - \n",
    "# otherwise a new set of variables for vgg16 will be defined\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    scope.reuse_variables()\n",
    "    style_end_points = '''Fill your code here'''\n",
    "        \n",
    "# Forward pass the output tensors - make sure to reuse the variables - \n",
    "# otherwise a new set of variables for vgg16 will be defined\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    scope.reuse_variables()\n",
    "    output_end_points = '''Fill your code here'''\n",
    "        \n",
    "\n",
    "#################################################################################    \n",
    "''' Loss computation '''\n",
    "#################################################################################    \n",
    "content_layers = ['conv1_2']\n",
    "style_layers = ['conv1_2', 'conv2_2', 'conv3_2', 'conv4_2', 'conv5_2']\n",
    "    \n",
    "    \n",
    "# Content loss\n",
    "content_loss = 0\n",
    "for layer in content_layers:\n",
    "    # MSE of the activations \n",
    "    mse = tf.losses.mean_squared_error(content_end_points[layer], output_end_points[layer])\n",
    "    # Add it to the content_loss \n",
    "    content_loss += mse\n",
    "        \n",
    "# Style loss\n",
    "style_loss = 0\n",
    "for layer in style_layers:\n",
    "    # Compute gram matrix of the activations for style and output\n",
    "    gram_matrix_style = gram_matrix(style_end_points[layer])\n",
    "    gram_matrix_output = gram_matrix(output_end_points[layer])\n",
    "        \n",
    "    # MSE of the gram matrix\n",
    "    '''Fill your code here'''\n",
    "        \n",
    "    # Add it to the style_loss \n",
    "    '''Fill your code here'''\n",
    "        \n",
    "# Combine the content and style losses - one can also weigh them differently\n",
    "total_loss = '''Fill your code here'''    \n",
    "    \n",
    "#################################################################################    \n",
    "'''Optimizers '''\n",
    "#################################################################################    \n",
    "# Define optimizers and pass only output_tensor as variable to be trained\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer_step = optimizer.minimize(total_loss, var_list=[output_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################    \n",
    "'''\n",
    "Create a TF session and execute the graph\n",
    "'''\n",
    "#################################################################################    \n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "'''Initialize and loading the weights'''\n",
    "#Initialize the variables and load the weights for VGG\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vgg.load_weights(FLAGS.models_path,sess)\n",
    "    \n",
    "    \n",
    "#################################################################################    \n",
    "'''\n",
    "Execute the graph several times to get the desired output\n",
    "'''   \n",
    "#################################################################################    \n",
    "for i in range(FLAGS.max_steps):\n",
    "    # sess.run() should consist of 'fetch' and 'feed' \n",
    "    # 'fetch' are the list of variables to be returned\n",
    "    # 'feed' is the dictionary of values assigned to respective placeholders\n",
    "    fetch = [optimizer_step, total_loss, content_loss, style_loss, output_tensor]\n",
    "    feed = '''Fill your code here'''\n",
    "    # Define the session.run() with fetch and feed parameters\n",
    "    _, loss, c_loss, s_loss, output_image = '''Fill your code here'''\n",
    "    print('Iteration : %4d, total_loss : %g, content_loss : %g, style_loss : %g' % (i,loss, c_loss, s_loss))\n",
    "    if i%10==0:\n",
    "        plt.imshow(np.hstack([content_image[0],output_image[0],style_image[0] ])/output_image.max()); plt.show()\n",
    "    print('Completed')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
