{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import pdb\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import model \n",
    "    \n",
    "flags = tf.flags\n",
    "flags.DEFINE_string(\"models_path\",'vgg16_weights.npz', \"\")\n",
    "flags.DEFINE_string(\"content_image_path\",'images/tubingen.jpg', \"\")\n",
    "flags.DEFINE_string(\"style_image_path\",'images/starry-night.jpg', \"\")\n",
    "flags.DEFINE_float(\"content_loss_weight\",0.999, \"\")\n",
    "flags.DEFINE_float(\"style_loss_weight\",1e-3, \"\")\n",
    "flags.DEFINE_integer(\"max_steps\",100, \"\")\n",
    "flags.DEFINE_float(\"learning_rate\",0.1, \"\")\n",
    "FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "VGG16 is a pretrained model which has been trained on 1.2million images\n",
    "Given below is the architecture of VGG16\n",
    "conv referes to convolutional layers \n",
    "fc refers to fully connected layers\n",
    "'''\n",
    "#################################################################################\n",
    "vgg_layers = ['conv1_1', 'conv1_2', 'pool1', \n",
    "              'conv2_1', 'conv2_2', 'pool2', \n",
    "              'conv3_1', 'conv3_2', 'conv3_3', 'pool3',\n",
    "              'conv4_1', 'conv4_2', 'conv4_3', 'pool4',\n",
    "              'conv5_1', 'conv5_2', 'conv5_3', 'pool5',\n",
    "              'fc6', 'fc7', 'fc8']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "Function which computes the gram matrix given a vector tensor \n",
    "'''\n",
    "#################################################################################\n",
    "def gram_matrix(feature_maps):\n",
    "  \"\"\"Computes the Gram matrix for a set of feature maps.\"\"\"\n",
    "  batch_size, height, width, channels = tf.unstack(tf.shape(feature_maps))\n",
    "  denominator = tf.to_float(height * width)\n",
    "  feature_maps = tf.reshape(\n",
    "      feature_maps, tf.stack([batch_size, height * width, channels]))\n",
    "  matrix = tf.matmul(feature_maps, feature_maps, adjoint_a=True)\n",
    "  return matrix / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "'''\n",
    "Read the content and style images \n",
    "The images are of shape: 224,224,3 #RGB channels \n",
    "'''\n",
    "#################################################################################\n",
    "    \n",
    "# Open content image and resize it \n",
    "content_image = imread(FLAGS.content_image_path)[:,:,:3]\n",
    "content_image = np.expand_dims(imresize(content_image, (224, 224)), 0)\n",
    "    \n",
    "# Open style image and resize it \n",
    "style_image = imread(FLAGS.style_image_path)[:,:,:3]\n",
    "style_image = np.expand_dims(imresize(style_image, (224, 224)), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 conv1_1_W (3, 3, 3, 64)\n",
      "1 conv1_1_b (64,)\n",
      "2 conv1_2_W (3, 3, 64, 64)\n",
      "3 conv1_2_b (64,)\n",
      "4 conv2_1_W (3, 3, 64, 128)\n",
      "5 conv2_1_b (128,)\n",
      "6 conv2_2_W (3, 3, 128, 128)\n",
      "7 conv2_2_b (128,)\n",
      "8 conv3_1_W (3, 3, 128, 256)\n",
      "9 conv3_1_b (256,)\n",
      "10 conv3_2_W (3, 3, 256, 256)\n",
      "11 conv3_2_b (256,)\n",
      "12 conv3_3_W (3, 3, 256, 256)\n",
      "13 conv3_3_b (256,)\n",
      "14 conv4_1_W (3, 3, 256, 512)\n",
      "15 conv4_1_b (512,)\n",
      "16 conv4_2_W (3, 3, 512, 512)\n",
      "17 conv4_2_b (512,)\n",
      "18 conv4_3_W (3, 3, 512, 512)\n",
      "19 conv4_3_b (512,)\n",
      "20 conv5_1_W (3, 3, 512, 512)\n",
      "21 conv5_1_b (512,)\n",
      "22 conv5_2_W (3, 3, 512, 512)\n",
      "23 conv5_2_b (512,)\n",
      "24 conv5_3_W (3, 3, 512, 512)\n",
      "25 conv5_3_b (512,)\n",
      "0 5.4729e+06 0.0 5.4729e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "1 5.39997e+06 17.3198 5.39995e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "2 5.32714e+06 70.2831 5.32707e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "3 5.25464e+06 160.443 5.25448e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "4 5.18242e+06 285.908 5.18214e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "5 5.11069e+06 440.851 5.11025e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "6 5.03952e+06 620.112 5.0389e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "7 4.96891e+06 815.734 4.9681e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "8 4.89883e+06 1024.21 4.8978e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "9 4.82925e+06 1240.25 4.82801e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "10 4.76009e+06 1461.47 4.75863e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "11 4.69129e+06 1685.26 4.6896e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "12 4.62286e+06 1911.73 4.62095e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "13 4.55476e+06 2141.23 4.55262e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "14 4.48699e+06 2371.7 4.48462e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "15 4.41957e+06 2602.25 4.41697e+09\n",
      "112.461 112.461581898 95.3338315795\n",
      "16 4.3525e+06 2832.84 4.34967e+09\n",
      "112.46 112.461581898 95.3338315795\n",
      "17 4.2858e+06 3062.83 4.28274e+09\n",
      "112.46 112.461581898 95.3338315795\n",
      "18 4.21948e+06 3293.81 4.21619e+09\n",
      "112.46 112.461581898 95.3338315795\n",
      "19 4.15354e+06 3526.91 4.15002e+09\n",
      "112.46 112.461581898 95.3338315795\n",
      "20 4.08804e+06 3761.49 4.08428e+09\n",
      "112.459 112.461581898 95.3338315795\n",
      "21 4.02298e+06 3996.56 4.01899e+09\n",
      "112.459 112.461581898 95.3338315795\n",
      "22 3.95838e+06 4232.21 3.95415e+09\n",
      "112.459 112.461581898 95.3338315795\n",
      "23 3.89429e+06 4469.43 3.88982e+09\n",
      "112.459 112.461581898 95.3338315795\n",
      "24 3.83071e+06 4707.85 3.826e+09\n",
      "112.458 112.461581898 95.3338315795\n",
      "25 3.76768e+06 4947.33 3.76274e+09\n",
      "112.458 112.461581898 95.3338315795\n",
      "26 3.70525e+06 5187.79 3.70007e+09\n",
      "112.457 112.461581898 95.3338315795\n",
      "27 3.64344e+06 5428.8 3.63802e+09\n",
      "112.457 112.461581898 95.3338315795\n",
      "28 3.58228e+06 5671.54 3.57661e+09\n",
      "112.457 112.461581898 95.3338315795\n",
      "29 3.52174e+06 5915.59 3.51583e+09\n",
      "112.456 112.461581898 95.3338315795\n",
      "30 3.46191e+06 6160.83 3.45575e+09\n",
      "112.456 112.461581898 95.3338315795\n",
      "31 3.40279e+06 6407.55 3.39639e+09\n",
      "112.455 112.461581898 95.3338315795\n",
      "32 3.34443e+06 6655.02 3.33778e+09\n",
      "112.455 112.461581898 95.3338315795\n",
      "33 3.28686e+06 6903.57 3.27997e+09\n",
      "112.454 112.461581898 95.3338315795\n",
      "34 3.2301e+06 7152.7 3.22295e+09\n",
      "112.454 112.461581898 95.3338315795\n",
      "35 3.17418e+06 7402.19 3.16679e+09\n",
      "112.453 112.461581898 95.3338315795\n",
      "36 3.11909e+06 7652.02 3.11144e+09\n",
      "112.453 112.461581898 95.3338315795\n",
      "37 3.06484e+06 7901.99 3.05695e+09\n",
      "112.452 112.461581898 95.3338315795\n",
      "38 3.01152e+06 8151.35 3.00337e+09\n",
      "112.451 112.461581898 95.3338315795\n",
      "39 2.95911e+06 8399.7 2.95072e+09\n",
      "112.451 112.461581898 95.3338315795\n",
      "40 2.90764e+06 8647.26 2.899e+09\n",
      "112.45 112.461581898 95.3338315795\n",
      "41 2.85713e+06 8893.53 2.84825e+09\n",
      "112.45 112.461581898 95.3338315795\n",
      "42 2.80761e+06 9138.62 2.79848e+09\n",
      "112.449 112.461581898 95.3338315795\n",
      "43 2.75906e+06 9382.33 2.74969e+09\n",
      "112.448 112.461581898 95.3338315795\n",
      "44 2.71154e+06 9623.73 2.70193e+09\n",
      "112.447 112.461581898 95.3338315795\n",
      "45 2.66504e+06 9862.55 2.65518e+09\n",
      "112.447 112.461581898 95.3338315795\n",
      "46 2.61956e+06 10098.8 2.60947e+09\n",
      "112.446 112.461581898 95.3338315795\n",
      "47 2.57512e+06 10332.7 2.5648e+09\n",
      "112.445 112.461581898 95.3338315795\n",
      "48 2.53166e+06 10564.2 2.52111e+09\n",
      "112.444 112.461581898 95.3338315795\n",
      "49 2.48923e+06 10792.6 2.47845e+09\n",
      "112.444 112.461581898 95.3338315795\n",
      "50 2.44782e+06 11017.3 2.43682e+09\n",
      "112.443 112.461581898 95.3338315795\n",
      "51 2.40741e+06 11239.2 2.39618e+09\n",
      "112.442 112.461581898 95.3338315795\n",
      "52 2.368e+06 11458.5 2.35655e+09\n",
      "112.441 112.461581898 95.3338315795\n",
      "53 2.32959e+06 11674.8 2.31792e+09\n",
      "112.44 112.461581898 95.3338315795\n",
      "54 2.29213e+06 11888.3 2.28025e+09\n",
      "112.439 112.461581898 95.3338315795\n",
      "55 2.25564e+06 12098.8 2.24356e+09\n",
      "112.438 112.461581898 95.3338315795\n",
      "56 2.22009e+06 12306.7 2.2078e+09\n",
      "112.437 112.461581898 95.3338315795\n",
      "57 2.18547e+06 12512.1 2.17297e+09\n",
      "112.436 112.461581898 95.3338315795\n",
      "58 2.15177e+06 12714.3 2.13907e+09\n",
      "112.435 112.461581898 95.3338315795\n",
      "59 2.11899e+06 12913.1 2.10609e+09\n",
      "112.434 112.461581898 95.3338315795\n",
      "60 2.08707e+06 13108.9 2.07397e+09\n",
      "112.433 112.461581898 95.3338315795\n",
      "61 2.05603e+06 13301.7 2.04274e+09\n",
      "112.432 112.461581898 95.3338315795\n",
      "62 2.02584e+06 13491.9 2.01236e+09\n",
      "112.43 112.461581898 95.3338315795\n",
      "63 1.99647e+06 13679.3 1.98281e+09\n",
      "112.429 112.461581898 95.3338315795\n",
      "64 1.96793e+06 13864.0 1.95408e+09\n",
      "112.428 112.461581898 95.3338315795\n",
      "65 1.94019e+06 14045.9 1.92616e+09\n",
      "112.427 112.461581898 95.3338315795\n",
      "66 1.9132e+06 14225.4 1.89899e+09\n",
      "112.426 112.461581898 95.3338315795\n",
      "67 1.88696e+06 14402.5 1.87257e+09\n",
      "112.424 112.461581898 95.3338315795\n",
      "68 1.86144e+06 14576.9 1.84687e+09\n",
      "112.423 112.461581898 95.3338315795\n",
      "69 1.8366e+06 14749.2 1.82186e+09\n",
      "112.422 112.461581898 95.3338315795\n",
      "70 1.81243e+06 14919.4 1.79753e+09\n",
      "112.421 112.461581898 95.3338315795\n",
      "71 1.78892e+06 15086.9 1.77385e+09\n",
      "112.419 112.461581898 95.3338315795\n",
      "72 1.76605e+06 15252.2 1.75081e+09\n",
      "112.418 112.461581898 95.3338315795\n",
      "73 1.74379e+06 15414.8 1.7284e+09\n",
      "112.417 112.461581898 95.3338315795\n",
      "74 1.72214e+06 15574.7 1.70658e+09\n",
      "112.415 112.461581898 95.3338315795\n",
      "75 1.70106e+06 15732.0 1.68535e+09\n",
      "112.414 112.461581898 95.3338315795\n",
      "76 1.68056e+06 15886.5 1.66469e+09\n",
      "112.413 112.461581898 95.3338315795\n",
      "77 1.6606e+06 16038.6 1.64457e+09\n",
      "112.411 112.461581898 95.3338315795\n",
      "78 1.64115e+06 16188.2 1.62498e+09\n",
      "112.41 112.461581898 95.3338315795\n",
      "79 1.62221e+06 16335.3 1.60589e+09\n",
      "112.409 112.461581898 95.3338315795\n",
      "80 1.60374e+06 16480.4 1.58728e+09\n",
      "112.407 112.461581898 95.3338315795\n",
      "81 1.58572e+06 16623.4 1.56911e+09\n",
      "112.406 112.461581898 95.3338315795\n",
      "82 1.56814e+06 16764.5 1.55139e+09\n",
      "112.404 112.461581898 95.3338315795\n",
      "83 1.55098e+06 16903.6 1.5341e+09\n",
      "112.403 112.461581898 95.3338315795\n",
      "84 1.53424e+06 17040.5 1.51721e+09\n",
      "112.402 112.461581898 95.3338315795\n",
      "85 1.51788e+06 17175.6 1.50072e+09\n",
      "112.4 112.461581898 95.3338315795\n",
      "86 1.50191e+06 17308.8 1.48462e+09\n",
      "112.399 112.461581898 95.3338315795\n",
      "87 1.48629e+06 17440.0 1.46887e+09\n",
      "112.397 112.461581898 95.3338315795\n",
      "88 1.47101e+06 17569.3 1.45346e+09\n",
      "112.396 112.461581898 95.3338315795\n",
      "89 1.45606e+06 17696.5 1.43838e+09\n",
      "112.394 112.461581898 95.3338315795\n",
      "90 1.44144e+06 17821.9 1.42363e+09\n",
      "112.393 112.461581898 95.3338315795\n",
      "91 1.42712e+06 17945.5 1.40919e+09\n",
      "112.391 112.461581898 95.3338315795\n",
      "92 1.41311e+06 18066.9 1.39506e+09\n",
      "112.39 112.461581898 95.3338315795\n",
      "93 1.39939e+06 18186.0 1.38122e+09\n",
      "112.389 112.461581898 95.3338315795\n",
      "94 1.38594e+06 18303.0 1.36765e+09\n",
      "112.387 112.461581898 95.3338315795\n",
      "95 1.37274e+06 18418.3 1.35434e+09\n",
      "112.386 112.461581898 95.3338315795\n",
      "96 1.3598e+06 18531.9 1.34129e+09\n",
      "112.384 112.461581898 95.3338315795\n",
      "97 1.3471e+06 18643.8 1.32848e+09\n",
      "112.383 112.461581898 95.3338315795\n",
      "98 1.33463e+06 18753.8 1.3159e+09\n",
      "112.381 112.461581898 95.3338315795\n",
      "99 1.32238e+06 18862.3 1.30354e+09\n",
      "112.38 112.461581898 95.3338315795\n",
      "Completed\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "'''\n",
    "Create Graph first by defining placeholders, ops, loss functions and optimizers\n",
    "\n",
    "'''\n",
    "#################################################################################\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#################################################################################\n",
    "'''Placeholders'''\n",
    "#################################################################################\n",
    "content_tensor = '''Fill your code here''' #shape=[1,224,224,3] \n",
    "style_tensor = '''Fill your code here''' #shape=[1,224,224,3] \n",
    "    \n",
    "                   \n",
    "# Define the output tensor as a Variable - so we can train it\n",
    "# The output tensor should be initialized with the content image\n",
    "#################################################################################\n",
    "'''Output Tensor - this is our output image which we will train'''\n",
    "#################################################################################    \n",
    "output_tensor = tf.Variable('''Fill your code here''') #shape=[1,224,224,3] \n",
    "    \n",
    "\n",
    "#################################################################################\n",
    "'''\n",
    "The class object for the VGG16 neural network\n",
    "Propagate the content_tensor, style_tensor and the output tensor through this model\n",
    "'''\n",
    "#################################################################################    \n",
    "vgg = model.vgg16()\n",
    "        \n",
    "# Forward pass the content tensors\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    content_end_points = vgg.forward(content_tensor)\n",
    "    # content_end_points are a dictionary of (key, value) pairs with \n",
    "    # keys=layer_names and values=activationsis a dictionary of \n",
    "        \n",
    "# Forward pass the style tensors - make sure to reuse the variables - \n",
    "# otherwise a new set of variables for vgg16 will be defined\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    scope.reuse_variables()\n",
    "    style_end_points = '''Fill your code here'''\n",
    "        \n",
    "# Forward pass the output tensors - make sure to reuse the variables - \n",
    "# otherwise a new set of variables for vgg16 will be defined\n",
    "with tf.variable_scope('vgg') as scope:\n",
    "    scope.reuse_variables()\n",
    "    output_end_points = '''Fill your code here'''\n",
    "        \n",
    "\n",
    "#################################################################################    \n",
    "''' Loss computation '''\n",
    "#################################################################################    \n",
    "content_layers = ['conv1_2']\n",
    "style_layers = ['conv1_2', 'conv2_2', 'conv3_2', 'conv4_2', 'conv5_2']\n",
    "    \n",
    "    \n",
    "# Content loss\n",
    "content_loss = 0\n",
    "for layer in content_layers:\n",
    "    # MSE of the activations \n",
    "    mse = tf.losses.mean_squared_error(content_end_points[layer], output_end_points[layer])\n",
    "    # Add it to the content_loss \n",
    "    content_loss.append(mse)\n",
    "        \n",
    "# Style loss\n",
    "style_loss = 0\n",
    "for layer in style_layers:\n",
    "    # Compute gram matrix of the activations for style and output\n",
    "    gram_matrix_style = gram_matrix(style_end_points[layer])\n",
    "    gram_matrix_output = gram_matrix(output_end_points[layer])\n",
    "        \n",
    "    # MSE of the gram matrix\n",
    "    '''Fill your code here'''\n",
    "        \n",
    "    # Add it to the style_loss \n",
    "    '''Fill your code here'''\n",
    "        \n",
    "# Combine the content and style losses - one can also weigh them differently\n",
    "total_loss = '''Fill your code here'''    \n",
    "    \n",
    "#################################################################################    \n",
    "'''Optimizers '''\n",
    "#################################################################################    \n",
    "# Define optimizers and pass only output_tensor as variable to be trained\n",
    "learning_rate = 0.1\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer_step = optimizer.minimize(total_loss, var_list=[output_tensor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################################    \n",
    "'''\n",
    "Create a TF session and execute the graph\n",
    "'''\n",
    "#################################################################################    \n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "'''Initialize and loading the weights'''\n",
    "Initialize the variables and load the weights for VGG\n",
    "sess.run(tf.global_variables_initializer())\n",
    "vgg.load_weights(FLAGS.models_path,sess)\n",
    "    \n",
    "    \n",
    "#################################################################################    \n",
    "'''\n",
    "Execute the graph several times to get the desired output\n",
    "'''   \n",
    "#################################################################################    \n",
    "for i in range(FLAGS.max_steps):\n",
    "    # sess.run() should consist of 'fetch' and 'feed' \n",
    "    # 'fetch' are the list of variables to be returned\n",
    "    # 'feed' is the dictionary of values assigned to respective placeholders\n",
    "    fetch = [optimizer_step, total_loss, content_loss, style_loss, output_tensor]\n",
    "    feed = '''Fill your code here'''\n",
    "    # Define the session.run() with fetch and feed parameters\n",
    "    _, loss, c_loss, s_loss, output_image = '''Fill your code here'''\n",
    "    print('Iteration : %4d, total_loss : %g, content_loss : %g, style_loss : %g' % (i,loss, c_loss, s_loss))\n",
    "    if i%10==0:\n",
    "        plt.imshow(np.hstack([content_image[0],output_image[0],style_image[0] ])/255.); plt.show()\n",
    "    print('Completed')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
